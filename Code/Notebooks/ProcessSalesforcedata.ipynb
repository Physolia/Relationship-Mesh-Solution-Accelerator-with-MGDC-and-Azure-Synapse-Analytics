{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = 'relmeshadlsfs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sf_accounts_path = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/salesforcedata/account/accounts.csv'\n",
        "mapping_file = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/column_mappings/Account_Column_Mapping_SF.csv'\n",
        "\n",
        "df_account_sf = spark.read.load(sf_accounts_path, format='csv',header=True)\n",
        "df_acc_map_sf = spark.read.load(mapping_file, format='csv',header=True).toPandas()\n",
        "\n",
        "col_map = dict(zip(df_acc_map_sf.SFColumn, df_acc_map_sf.SAColumn))\n",
        "\n",
        "df_account_sf = df_account_sf.select(*[col(k).alias(col_map[k]) for k in col_map])\n",
        "\n",
        "sa_acc_cols = ['Id','Name','DomainName','ParentAccount','PrimaryContact','Industry','Sector','TransactionSize','Tier','Type']\n",
        "df_account_sf = df_account_sf.select(*sa_acc_cols)\n",
        "#display(df_account_sf.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sf_contacts_path = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/salesforcedata/contact/contacts.csv'\n",
        "mapping_file = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/column_mappings/Contact_Column_Mapping_SF.csv'\n",
        "\n",
        "df_contact_sf = spark.read.load(sf_contacts_path, format='csv',header=True)\n",
        "df_contact_map_sf = spark.read.load(mapping_file, format='csv',header=True).toPandas()\n",
        "\n",
        "col_map = dict(zip(df_contact_map_sf.SFColumn, df_contact_map_sf.SAColumn))\n",
        "\n",
        "df_contact_sf = df_contact_sf.select(*[col(k).alias(col_map[k]) for k in col_map])\n",
        "\n",
        "sa_contact_cols = ['Id','AccountId','FirstName','LastName','Name','Email','Title','ContactType__c']\n",
        "df_contact_sf = df_contact_sf.select(*sa_contact_cols)\n",
        "# display(df_contact_sf.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_acc = df_account_sf.filter(df_account_sf.Type == 'Account').select('Id','Name','DomainName','ParentAccount','PrimaryContact','Industry','Sector','TransactionSize','Tier')\n",
        "df_parent_acc = df_account_sf.filter(df_account_sf.Type == 'ParentAccount').select('Id','Name','DomainName')\n",
        "df_acc.write.mode(\"overwrite\").saveAsTable(\"account\")\n",
        "df_parent_acc.write.mode(\"overwrite\").saveAsTable(\"parentaccount\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_contact = df_contact_sf.filter(df_contact_sf.ContactType__c == 'Contact').select('Id','AccountId','FirstName','LastName','Name','Email','Title')\n",
        "df_employee = df_contact_sf.filter(df_contact_sf.ContactType__c == 'Employee').select('Id','FirstName','LastName','Name','Email','Title')\n",
        "\n",
        "df_contact = df_contact.withColumn('Exec_Flag', \\\n",
        "    when((col(\"Title\") == 'CEO') | (col(\"Title\") == 'CTO') | (col(\"Title\") == 'CIO') | (col(\"Title\") == 'CFO'), 1) \\\n",
        "    .otherwise(0)\n",
        ")\n",
        "\n",
        "df_contact.write.mode(\"overwrite\").saveAsTable(\"contact\")\n",
        "df_employee.write.mode(\"overwrite\").saveAsTable(\"employee\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sql_stmt = '''\n",
        "    Select c.AccountId as Account_Id, a.Name as Account_Name,\n",
        "    a.PrimaryContact as Account_Primary_Caller__c, 'Q1' as Account_Timing_Quarter__c, \n",
        "    '2021' as Account_Timing__c, 'Tier1' as Account_Industry_Tier__c,\n",
        "    a.Industry, a.Sector,\n",
        "    c.Id as Contact_Id, c.FirstName, c.LastName, c.Name as Full_Name__c,Title, lower(Email) as Email,\n",
        "    0 as IsBoardMember, Exec_Flag as IsExec\n",
        "    from account as a \n",
        "    inner join contact as c on a.Id = c.AccountId\n",
        "    where Email is not null\n",
        "'''\n",
        "df = spark.sql(sql_stmt)\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"SellersExtContacts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sql_stmt = '''\n",
        "SELECT Contact_Id, Full_Name__c, Title,IsBoardMember,IsExec\n",
        "from SellersExtContacts\n",
        "where Account_Id is not null\n",
        "'''\n",
        "\n",
        "df = spark.sql(sql_stmt)\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"ExtContacts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sql_stmt = '''select  a.Id as Account_Id, PrimaryContact as Contact_Id,\n",
        "            c.Name as Full_Name__c, lower(Email) as Email\n",
        "            FROM Account as a \n",
        "            inner join Employee as c on a.PrimaryContact = c.Id'''\n",
        "\n",
        "df_primarycallers = spark.sql(sql_stmt)\n",
        "df_primarycallers.write.mode(\"overwrite\").saveAsTable(\"PrimaryCallers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#HPTs list\n",
        "sql_stmt = '''SELECT a.Id,Name,Industry,Sector,'Tier1' as Industry_Tier__c,'2021' as Timing__c, 'Q1' as Timing_Quarter__c,\n",
        "            '10000' as Est_Transaction_Size__c,\n",
        "\t\t\tp.Contact_Id as Primary_Caller_Id,p.Full_Name__C as Primary_Caller\n",
        "\t\t\tFROM Account as a \n",
        "\t\t\tinner join PrimaryCallers as p on p.Account_Id = a.Id'''\n",
        "\n",
        "df_netgraphAccounts = spark.sql(sql_stmt)\n",
        "df_netgraphAccounts.write.mode(\"overwrite\").saveAsTable(\"HighPriorityTargets\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
