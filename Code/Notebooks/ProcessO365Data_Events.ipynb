{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "inputFolderPath = '2021/11/2/2021-11-02T03:55:08'\n",
        "initialLoad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if initialLoad == True:\n",
        "    spark.sql('drop table if exists events_temp')\n",
        "    spark.sql('drop table if exists eventsdata') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "def get_location_flag(Location_DisplayName): \n",
        "    location_flag = 'InPerson'\n",
        "    if 'teams' in Location_DisplayName or 'zoom' in Location_DisplayName \\\n",
        "    or 'webex' in Location_DisplayName or 'loopup' in Location_DisplayName \\\n",
        "    or 'dial' in Location_DisplayName or 'conference' in Location_DisplayName  \\\n",
        "    or '+1' in Location_DisplayName or 'meeting' in Location_DisplayName  \\\n",
        "    or 'cell' in Location_DisplayName or '1-8' in Location_DisplayName  \\\n",
        "    or 'code' in Location_DisplayName or '888' in Location_DisplayName  \\\n",
        "    or '800' in Location_DisplayName or 'call' in Location_DisplayName  \\\n",
        "    or '#' in Location_DisplayName or 'google meet' in Location_DisplayName :\n",
        "        location_flag = 'Remote'\n",
        "    return location_flag\n",
        "    \n",
        "def load_events_json_file(eventsPath):\n",
        "    events_df= spark.read.load(eventsPath, format='json')\n",
        "\n",
        "    location_flag_udf = udf(lambda Location_DisplayName: get_location_flag(Location_DisplayName), returnType=StringType())\n",
        "\n",
        "    df = events_df.select('*',size('Attendees').alias('Attendees_cnt')) \\\n",
        "                .select(explode(col(\"Attendees\")).alias(\"Attendees\"),\"Id\",\"puser\",\"ICalUid\",\"Subject\",\"Recurrence\",\"IsCancelled\",\"Start\",\"End\",\"Organizer\",\"Attendees_cnt\",\"Location\") \\\n",
        "                .select([\"Id\",\"puser\",\"ICalUid\",\"Subject\",\"Attendees_cnt\",\n",
        "                            when(col(\"Recurrence\").isNull(),False).otherwise(True).alias(\"Recurrence\"),\n",
        "                            \"IsCancelled\",\n",
        "                            col(\"Start.DateTime\").alias(\"Start\"),col(\"End.DateTime\").alias(\"End\"),\n",
        "                            col(\"Organizer.EmailAddress.Address\").alias(\"Organizer\"),\n",
        "                            col(\"Attendees.EmailAddress.Address\").alias(\"Attendee\"),\n",
        "                            col(\"Attendees.Status.Response\").alias(\"Attendee_Response\"),\n",
        "                            col(\"Attendees.Type\").alias(\"Attendee_Type\"),\n",
        "                            col(\"Location.Address.Type\").alias(\"Location_Address_Type\"),\n",
        "                            col(\"Location.DisplayName\").alias(\"Location_DisplayName\")]) \\\n",
        "                .withColumn('location_flag', location_flag_udf(lower(col(\"Location_DisplayName\")))) \\\n",
        "                .withColumn(\"LoadFolderPath\", lit(inputFolderPath))\n",
        "\n",
        "    df.write.mode(\"append\").saveAsTable(\"events_temp\") \n",
        "    \n",
        "def get_event_subfolder_files(folder):\n",
        "    children = mssparkutils.fs.ls(folder)\n",
        "    #print(children[0])\n",
        "    for child in children:\n",
        "        if child.name == 'metadata':\n",
        "            continue\n",
        "        if child.isDir:\n",
        "            get_event_subfolder_files(child.path)\n",
        "        else:\n",
        "            load_events_json_file(child.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "eventsPath = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/o365data/events/\"\n",
        "get_event_subfolder_files(eventsPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sql_str = '''SELECT  Id,puser,ICalUid,Subject,Recurrence,IsCancelled,\n",
        "to_timestamp(Start) as Start,\n",
        "to_date(Start) as Start_Date,\n",
        "to_timestamp(End) as End,\n",
        "lower(Organizer) as Organizer,lower(Attendee) as Attendee,\n",
        "Attendee_Response,Attendee_Type,\n",
        "reverse(split(lower(Organizer),'@'))[0] as Organizer_Domain,\n",
        "reverse(split(lower(Attendee),'@'))[0] as Attendee_Domain,\n",
        "Attendees_cnt,location_flag\n",
        "FROM events_temp'''\n",
        "\n",
        "sql_str = sql_str + \" where LoadFolderPath = '\" + inputFolderPath + \"'\" \n",
        "\n",
        "sql_str = sql_str + ' UNION ALL '\n",
        "\n",
        "sql_str = sql_str + '''SELECT  Id,puser,ICalUid,Subject,Recurrence,IsCancelled,\n",
        "to_timestamp(Start) as Start,\n",
        "to_date(Start) as Start_Date,\n",
        "to_timestamp(End) as End,\n",
        "lower(Attendee) as Organizer,lower(Organizer) as Attendee,\n",
        "Attendee_Response,Attendee_Type,\n",
        "reverse(split(lower(Attendee),'@'))[0] as Organizer_Domain,\n",
        "reverse(split(lower(Organizer),'@'))[0] as Attendee_Domain,\n",
        "Attendees_cnt,location_flag\n",
        "FROM events_temp\n",
        "'''\n",
        "sql_str = sql_str + \" where LoadFolderPath = '\" + inputFolderPath + \"'\"\n",
        "\n",
        "eventsdata_df = spark.sql(sql_str)\n",
        "eventsdata_df.write.mode(\"append\").saveAsTable(\"eventsdata\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
